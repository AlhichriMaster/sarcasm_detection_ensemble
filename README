#Sarcasm Detection using Neural Network Ensemble

This repository contains a TensorFlow implementation of an ensemble neural network for detecting sarcasm in news headlines. The project implements and compares three different neural architectures (LSTM, Attention, and Transformer) and combines them into an ensemble model for improved performance.

## Project Structure

```
sarcasm-detection/
├── data/
│   └── sarcasm_data_generator.py
├── models/
│   ├── ensemble_model.py
│   └── sarcasm_models.py
├── saved_models/
│   ├── lstm.weights.h5
│   ├── attention.weights.h5
│   └── transformer.weights.h5
├── results/
├── Sarcasm_Headlines_Dataset.json
├── main.py
├── demo.py
└── README.md
```

## Requirements

- Python 3.8+
- TensorFlow 2.4+
- NumPy
- scikit-learn
- JSON

You can install the required packages using:

```bash
pip install tensorflow numpy scikit-learn
```

## Dataset

The project uses the "Sarcasm Detection through NLP" dataset, which consists of news headlines labeled as either sarcastic (from TheOnion) or non-sarcastic (from HuffPost). The dataset is included as `Sarcasm_Headlines_Dataset.json`.

## File Descriptions

### Main Scripts

- `main.py`: The primary script for training and evaluating all models. This script:
  - Loads and prepares the dataset
  - Builds the individual models (LSTM, Attention, Transformer)
  - Trains each model and saves the weights
  - Creates and evaluates the ensemble model
  - Reports performance metrics for all models

- `demo.py`: A demonstration script that allows you to test the trained models on your own headlines. This script:
  - Loads pre-trained model weights
  - Accepts user input of headlines
  - Returns predictions from all models and the ensemble

### Model Implementation

- `models/ensemble_model.py`: Contains the `SarcasmEnsemble` class which:
  - Takes a list of trained models as input
  - Builds an ensemble model that averages the predictions from all input models

- `models/sarcasm_models.py`: Contains the `SarcasmModels` class which implements the three neural architectures:
  - `build_lstm_model()`: Creates an LSTM-based model with regularization
  - `build_attention_model()`: Creates a model with multi-head self-attention
  - `build_transformer_model()`: Creates a transformer-based model with configurable blocks
  - Helper methods for building transformer components

### Data Processing

- `data/sarcasm_data_generator.py` (not shown in provided files): This file likely contains the `SarcasmDataGenerator` class which:
  - Handles text preprocessing
  - Tokenizes and sequences the headlines
  - Creates TensorFlow data generators for batch training

## How to Run

### Training Models

To train all models from scratch and evaluate their performance:

```bash
python main.py
```

This will:
1. Load and preprocess the dataset
2. Train the individual models (LSTM, Attention, Transformer)
3. Save model weights to the `saved_models/` directory
4. Create and evaluate the ensemble model
5. Print evaluation metrics for all models
6. Save results to the `results/` directory

The training process might take several hours depending on your hardware.

### Running the Demo

To test the trained models on custom headlines:

```bash
python demo.py
```

This will:
1. Load the pre-trained models
2. Prompt you to enter news headlines
3. Display the prediction results from each model and the ensemble
4. For each headline, indicate whether it's predicted to be sarcastic and show the confidence score

Example usage:
```
Enter text: Scientists discover that drinking water prevents dehydration
```

### Customizing Models

You can modify hyperparameters in `main.py` to experiment with different model configurations:

- LSTM units and layers
- Attention heads and dimensions
- Transformer blocks and feed-forward network size
- Learning rate, batch size, and epochs
- Dropout rates and regularization

## Visualizing Results

The training and evaluation metrics are printed to the console during execution. For more detailed analysis, the results can be exported as CSV files or visualized using libraries like Matplotlib (you would need to add visualization code).

## Performance

With optimized hyperparameters, the ensemble model achieves approximately 85.8% accuracy on the test set, outperforming all individual models:

- LSTM: 84.7% accuracy
- Attention: 84.0% accuracy  
- Transformer: 82.4% accuracy
- Ensemble: 85.8% accuracy




# 1. Prerequisites
sudo pacman -S docker aws-cli-v2
sudo systemctl start docker
newgrp docker   # or log out/in

# 2. Fill in credentials
nano terraform/main.tf   # set aws_access_key and aws_secret_key

# 3. Deploy (takes ~10-15 min for first Docker build)
cd terraform
terraform init
terraform apply
The API URL is printed as output after apply completes.